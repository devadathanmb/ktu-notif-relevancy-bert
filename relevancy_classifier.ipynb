{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXKw7XsqDpHE",
        "outputId": "3636cb24-492a-4cfe-b58e-ab96b044f4ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 32/32 [00:02<00:00, 13.67it/s]\n",
            "Epoch 1: 100%|██████████| 32/32 [00:02<00:00, 14.06it/s]\n",
            "Epoch 2: 100%|██████████| 32/32 [00:02<00:00, 13.94it/s]\n",
            "Epoch 3: 100%|██████████| 32/32 [00:02<00:00, 13.88it/s]\n",
            "Epoch 4: 100%|██████████| 32/32 [00:02<00:00, 13.89it/s]\n",
            "Epoch 5: 100%|██████████| 32/32 [00:02<00:00, 13.90it/s]\n",
            "Epoch 6: 100%|██████████| 32/32 [00:02<00:00, 13.80it/s]\n",
            "Epoch 7: 100%|██████████| 32/32 [00:02<00:00, 13.70it/s]\n",
            "Epoch 8: 100%|██████████| 32/32 [00:02<00:00, 13.67it/s]\n",
            "Epoch 9: 100%|██████████| 32/32 [00:02<00:00, 13.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 1\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Load your dataset\n",
        "with open('data.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained DistilBERT tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_data(data):\n",
        "    return tokenizer([item[\"subject\"] for item in data], truncation=True, padding=True)\n",
        "\n",
        "train_encodings = tokenize_data(train_data)\n",
        "val_encodings = tokenize_data(val_data)\n",
        "\n",
        "# Convert lists to PyTorch tensors\n",
        "train_encodings = {key: torch.tensor(val) for key, val in train_encodings.items()}\n",
        "val_encodings = {key: torch.tensor(val) for key, val in val_encodings.items()}\n",
        "\n",
        "# Create PyTorch datasets\n",
        "train_labels = torch.tensor([item[\"relevant\"] for item in train_data], dtype=torch.long)\n",
        "val_labels = torch.tensor([item[\"relevant\"] for item in val_data], dtype=torch.long)\n",
        "train_dataset = TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], train_labels)\n",
        "val_dataset = TensorDataset(val_encodings[\"input_ids\"], val_encodings[\"attention_mask\"], val_labels)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = {\n",
        "    'output_dir': './results',\n",
        "    'num_train_epochs': 10,\n",
        "    'per_device_train_batch_size': 8,\n",
        "    'per_device_eval_batch_size': 8,\n",
        "    'warmup_steps': 500,\n",
        "    'weight_decay': 0.01,\n",
        "}\n",
        "\n",
        "# Create a PyTorch DataLoader for training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=training_args['per_device_train_batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=training_args['per_device_eval_batch_size'], shuffle=False)\n",
        "\n",
        "# Training\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(training_args['num_train_epochs']):\n",
        "    for batch in tqdm(train_loader, desc=f'Epoch {epoch}'):\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        loss = torch.nn.functional.cross_entropy(outputs.logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Save the trained model\n",
        "model.save_pretrained('./trained_model')\n",
        "# Load the trained model for inference\n",
        "model = DistilBertForSequenceClassification.from_pretrained('./trained_model')\n",
        "model.to(device)  # Ensure the model is on the same device as input tensors\n",
        "\n",
        "# Test on custom input\n",
        "custom_input = \"BTech result published\"\n",
        "tokenized_input = tokenizer(custom_input, return_tensors='pt')\n",
        "input_ids = tokenized_input['input_ids'].to(device)\n",
        "attention_mask = tokenized_input['attention_mask'].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "logits = outputs.logits\n",
        "predicted_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "print(f'Predicted Class: {predicted_class}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the trained model for inference\n",
        "model = DistilBertForSequenceClassification.from_pretrained('./trained_model')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)  # Ensure the model is on the same device as input tensors\n",
        "\n",
        "# Test on custom input\n",
        "custom_input = \"classes postponed\"\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "tokenized_input = tokenizer(custom_input, return_tensors='pt')\n",
        "input_ids = tokenized_input['input_ids'].to(device)\n",
        "attention_mask = tokenized_input['attention_mask'].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "logits = outputs.logits\n",
        "predicted_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "print(f'Predicted Class: {predicted_class}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taimh4STGrkl",
        "outputId": "d990bf39-9d32-47fe-ad26-ed4a1e6d4880"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 1\n"
          ]
        }
      ]
    }
  ]
}